{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_length(arr):\n",
    "    res = 0\n",
    "    for s in (0, 4):\n",
    "        res += ((arr[s] - arr[s + 2]) ** 2 +\n",
    "                (arr[s + 1] - arr[s + 3]) ** 2)\n",
    "    return res\n",
    "\n",
    "\n",
    "def min_length(arr):\n",
    "    res = 100\n",
    "    arr = arr[:8]\n",
    "    #numpyに\n",
    "    arr = np.array(arr)\n",
    "    for s in (0,4):\n",
    "        res = min(res, (arr[s] - arr[s + 2]) ** 2 +\n",
    "                  (arr[s + 1] - arr[s + 3]) ** 2)\n",
    "    print(f\"\\r{res}\",end=\"\")\n",
    "    return res\n",
    "\n",
    "def cherry_pick(df,threshold=10):\n",
    "    # data[:8]のmin_lengthが10未満のものをdrop\n",
    "    df = df.apply(lambda x: min_length(x) >= threshold, axis=0)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#memo: 長さを固定して、出発地点・角度(rad)を2値にする？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3665253307230564e-06"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dette\\AppData\\Local\\Temp\\ipykernel_13604\\837478395.py:11: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  arr = arr[:8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "200/200 [==============================] - 5s 24ms/step - loss: 0.7181 - mse: 0.7181 - val_loss: 0.6390 - val_mse: 0.6390\n",
      "Epoch 2/500\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5802 - mse: 0.5802 - val_loss: 0.6090 - val_mse: 0.6090\n",
      "Epoch 3/500\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5122 - mse: 0.5122 - val_loss: 0.5843 - val_mse: 0.5843\n",
      "Epoch 4/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4601 - mse: 0.4601 - val_loss: 0.5744 - val_mse: 0.5744\n",
      "Epoch 5/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4108 - mse: 0.4108 - val_loss: 0.5775 - val_mse: 0.5775\n",
      "Epoch 6/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.3681 - mse: 0.3681 - val_loss: 0.5677 - val_mse: 0.5677\n",
      "Epoch 7/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.3289 - mse: 0.3289 - val_loss: 0.5615 - val_mse: 0.5615\n",
      "Epoch 8/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2942 - mse: 0.2942 - val_loss: 0.5613 - val_mse: 0.5613\n",
      "Epoch 9/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2664 - mse: 0.2664 - val_loss: 0.5837 - val_mse: 0.5837\n",
      "Epoch 10/500\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 0.2388 - mse: 0.2388 - val_loss: 0.5743 - val_mse: 0.5743\n",
      "Epoch 11/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2181 - mse: 0.2181 - val_loss: 0.5698 - val_mse: 0.5698\n",
      "Epoch 12/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1972 - mse: 0.1972 - val_loss: 0.5846 - val_mse: 0.5846\n",
      "Epoch 13/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1781 - mse: 0.1781 - val_loss: 0.5839 - val_mse: 0.5839\n",
      "Epoch 14/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1627 - mse: 0.1627 - val_loss: 0.5776 - val_mse: 0.5776\n",
      "Epoch 15/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1520 - mse: 0.1520 - val_loss: 0.5796 - val_mse: 0.5796\n",
      "Epoch 16/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1373 - mse: 0.1373 - val_loss: 0.5837 - val_mse: 0.5837\n",
      "Epoch 17/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1308 - mse: 0.1308 - val_loss: 0.5862 - val_mse: 0.5862\n",
      "Epoch 18/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1230 - mse: 0.1230 - val_loss: 0.5909 - val_mse: 0.5909\n",
      "Epoch 19/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1144 - mse: 0.1144 - val_loss: 0.5789 - val_mse: 0.5789\n",
      "Epoch 20/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1082 - mse: 0.1082 - val_loss: 0.5884 - val_mse: 0.5884\n",
      "Epoch 21/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1062 - mse: 0.1062 - val_loss: 0.5795 - val_mse: 0.5795\n",
      "Epoch 22/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.5884 - val_mse: 0.5884\n",
      "Epoch 23/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.5807 - val_mse: 0.5807\n",
      "Epoch 24/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.5862 - val_mse: 0.5862\n",
      "Epoch 25/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0853 - mse: 0.0853 - val_loss: 0.5767 - val_mse: 0.5767\n",
      "Epoch 26/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0865 - mse: 0.0865 - val_loss: 0.5895 - val_mse: 0.5895\n",
      "Epoch 27/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0825 - mse: 0.0825 - val_loss: 0.5832 - val_mse: 0.5832\n",
      "Epoch 28/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0818 - mse: 0.0818 - val_loss: 0.5920 - val_mse: 0.5920\n",
      "Epoch 29/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0789 - mse: 0.0789 - val_loss: 0.5768 - val_mse: 0.5768\n",
      "Epoch 30/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0765 - mse: 0.0765 - val_loss: 0.5871 - val_mse: 0.5871\n",
      "Epoch 31/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0730 - mse: 0.0730 - val_loss: 0.5723 - val_mse: 0.5723\n",
      "Epoch 32/500\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0667 - mse: 0.0667 - val_loss: 0.5682 - val_mse: 0.5682\n",
      "Epoch 33/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0656 - mse: 0.0656 - val_loss: 0.5846 - val_mse: 0.5846\n",
      "Epoch 34/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0670 - mse: 0.0670 - val_loss: 0.5892 - val_mse: 0.5892\n",
      "Epoch 35/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0664 - mse: 0.0664 - val_loss: 0.5786 - val_mse: 0.5786\n",
      "Epoch 36/500\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0650 - mse: 0.0650 - val_loss: 0.5714 - val_mse: 0.5714\n",
      "Epoch 37/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0633 - mse: 0.0633 - val_loss: 0.5748 - val_mse: 0.5748\n",
      "Epoch 38/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0592 - mse: 0.0592 - val_loss: 0.5742 - val_mse: 0.5742\n",
      "Epoch 39/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0594 - mse: 0.0594 - val_loss: 0.5721 - val_mse: 0.5721\n",
      "Epoch 40/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0580 - mse: 0.0580 - val_loss: 0.5716 - val_mse: 0.5716\n",
      "Epoch 41/500\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.0595 - mse: 0.0595 - val_loss: 0.5676 - val_mse: 0.5676\n",
      "Epoch 42/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0570 - mse: 0.0570 - val_loss: 0.5757 - val_mse: 0.5757\n",
      "Epoch 43/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0527 - mse: 0.0527 - val_loss: 0.5776 - val_mse: 0.5776\n",
      "Epoch 44/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0538 - mse: 0.0538 - val_loss: 0.5823 - val_mse: 0.5823\n",
      "Epoch 45/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0549 - mse: 0.0549 - val_loss: 0.5654 - val_mse: 0.5654\n",
      "Epoch 46/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0539 - mse: 0.0539 - val_loss: 0.5724 - val_mse: 0.5724\n",
      "Epoch 47/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0504 - mse: 0.0504 - val_loss: 0.5662 - val_mse: 0.5662\n",
      "Epoch 48/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0505 - mse: 0.0505 - val_loss: 0.5731 - val_mse: 0.5731\n",
      "Epoch 49/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0502 - mse: 0.0502 - val_loss: 0.5784 - val_mse: 0.5784\n",
      "Epoch 50/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0491 - mse: 0.0491 - val_loss: 0.5743 - val_mse: 0.5743\n",
      "Epoch 51/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0488 - mse: 0.0488 - val_loss: 0.5683 - val_mse: 0.5683\n",
      "Epoch 52/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0458 - mse: 0.0458 - val_loss: 0.5730 - val_mse: 0.5730\n",
      "Epoch 53/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0439 - mse: 0.0439 - val_loss: 0.5672 - val_mse: 0.5672\n",
      "Epoch 54/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0441 - mse: 0.0441 - val_loss: 0.5671 - val_mse: 0.5671\n",
      "Epoch 55/500\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0447 - mse: 0.0447 - val_loss: 0.5718 - val_mse: 0.5718\n",
      "Epoch 56/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0448 - mse: 0.0448 - val_loss: 0.5635 - val_mse: 0.5635\n",
      "Epoch 57/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0437 - mse: 0.0437 - val_loss: 0.5693 - val_mse: 0.5693\n",
      "Epoch 58/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0452 - mse: 0.0452 - val_loss: 0.5657 - val_mse: 0.5657\n",
      "Epoch 59/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0439 - mse: 0.0439 - val_loss: 0.5695 - val_mse: 0.5695\n",
      "Epoch 60/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.5672 - val_mse: 0.5672\n",
      "Epoch 61/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0416 - mse: 0.0416 - val_loss: 0.5604 - val_mse: 0.5604\n",
      "Epoch 62/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.5792 - val_mse: 0.5792\n",
      "Epoch 63/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0416 - mse: 0.0416 - val_loss: 0.5651 - val_mse: 0.5651\n",
      "Epoch 64/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.5637 - val_mse: 0.5637\n",
      "Epoch 65/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.5614 - val_mse: 0.5614\n",
      "Epoch 66/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.5642 - val_mse: 0.5642\n",
      "Epoch 67/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.5696 - val_mse: 0.5696\n",
      "Epoch 68/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.5616 - val_mse: 0.5616\n",
      "Epoch 69/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.5660 - val_mse: 0.5660\n",
      "Epoch 70/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0373 - mse: 0.0373 - val_loss: 0.5660 - val_mse: 0.5660\n",
      "Epoch 71/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.5628 - val_mse: 0.5628\n",
      "Epoch 72/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.5695 - val_mse: 0.5695\n",
      "Epoch 73/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.5628 - val_mse: 0.5628\n",
      "Epoch 74/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0405 - mse: 0.0405 - val_loss: 0.5745 - val_mse: 0.5745\n",
      "Epoch 75/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.5623 - val_mse: 0.5623\n",
      "Epoch 76/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0359 - mse: 0.0359 - val_loss: 0.5677 - val_mse: 0.5677\n",
      "Epoch 77/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0348 - mse: 0.0348 - val_loss: 0.5645 - val_mse: 0.5645\n",
      "Epoch 78/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0327 - mse: 0.0327 - val_loss: 0.5587 - val_mse: 0.5587\n",
      "Epoch 79/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.5558 - val_mse: 0.5558\n",
      "Epoch 80/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.5599 - val_mse: 0.5599\n",
      "Epoch 81/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.5573 - val_mse: 0.5573\n",
      "Epoch 82/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.5582 - val_mse: 0.5582\n",
      "Epoch 83/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.5608 - val_mse: 0.5608\n",
      "Epoch 84/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0351 - mse: 0.0351 - val_loss: 0.5632 - val_mse: 0.5632\n",
      "Epoch 85/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0349 - mse: 0.0349 - val_loss: 0.5570 - val_mse: 0.5570\n",
      "Epoch 86/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.5654 - val_mse: 0.5654\n",
      "Epoch 87/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.5601 - val_mse: 0.5601\n",
      "Epoch 88/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.5551 - val_mse: 0.5551\n",
      "Epoch 89/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.5590 - val_mse: 0.5590\n",
      "Epoch 90/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.5622 - val_mse: 0.5622\n",
      "Epoch 91/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.5576 - val_mse: 0.5576\n",
      "Epoch 92/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.5650 - val_mse: 0.5650\n",
      "Epoch 93/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.5584 - val_mse: 0.5584\n",
      "Epoch 94/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.5606 - val_mse: 0.5606\n",
      "Epoch 95/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.5611 - val_mse: 0.5611\n",
      "Epoch 96/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0327 - mse: 0.0327 - val_loss: 0.5561 - val_mse: 0.5561\n",
      "Epoch 97/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.5590 - val_mse: 0.5590\n",
      "Epoch 98/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.5580 - val_mse: 0.5580\n",
      "Epoch 99/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.5654 - val_mse: 0.5654\n",
      "Epoch 100/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.5610 - val_mse: 0.5610\n",
      "Epoch 101/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.5579 - val_mse: 0.5579\n",
      "Epoch 102/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.5559 - val_mse: 0.5559\n",
      "Epoch 103/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.5550 - val_mse: 0.5550\n",
      "Epoch 104/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.5630 - val_mse: 0.5630\n",
      "Epoch 105/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.5574 - val_mse: 0.5574\n",
      "Epoch 106/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.5610 - val_mse: 0.5610\n",
      "Epoch 107/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.5563 - val_mse: 0.5563\n",
      "Epoch 108/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.5536 - val_mse: 0.5536\n",
      "Epoch 109/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.5538 - val_mse: 0.5538\n",
      "Epoch 110/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.5550 - val_mse: 0.5550\n",
      "Epoch 111/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.5563 - val_mse: 0.5563\n",
      "Epoch 112/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.5567 - val_mse: 0.5567\n",
      "Epoch 113/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.5590 - val_mse: 0.5590\n",
      "Epoch 114/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.5690 - val_mse: 0.5690\n",
      "Epoch 115/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.5516 - val_mse: 0.5516\n",
      "Epoch 116/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.5587 - val_mse: 0.5587\n",
      "Epoch 117/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.5554 - val_mse: 0.5554\n",
      "Epoch 118/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.5598 - val_mse: 0.5598\n",
      "Epoch 119/500\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.5590 - val_mse: 0.5590\n",
      "Epoch 120/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.5552 - val_mse: 0.5552\n",
      "Epoch 121/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.5559 - val_mse: 0.5559\n",
      "Epoch 122/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.5582 - val_mse: 0.5582\n",
      "Epoch 123/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.5516 - val_mse: 0.5516\n",
      "Epoch 124/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.5566 - val_mse: 0.5566\n",
      "Epoch 125/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0241 - mse: 0.0241 - val_loss: 0.5532 - val_mse: 0.5532\n",
      "Epoch 126/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.5639 - val_mse: 0.5639\n",
      "Epoch 127/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0236 - mse: 0.0236 - val_loss: 0.5602 - val_mse: 0.5602\n",
      "Epoch 128/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.5614 - val_mse: 0.5614\n",
      "Epoch 129/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.5605 - val_mse: 0.5605\n",
      "Epoch 130/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.5579 - val_mse: 0.5579\n",
      "Epoch 131/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.5585 - val_mse: 0.5585\n",
      "Epoch 132/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.5556 - val_mse: 0.5556\n",
      "Epoch 133/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0240 - mse: 0.0240 - val_loss: 0.5584 - val_mse: 0.5584\n",
      "Epoch 134/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0236 - mse: 0.0236 - val_loss: 0.5549 - val_mse: 0.5549\n",
      "Epoch 135/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0238 - mse: 0.0238 - val_loss: 0.5521 - val_mse: 0.5521\n",
      "Epoch 136/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0241 - mse: 0.0241 - val_loss: 0.5541 - val_mse: 0.5541\n",
      "Epoch 137/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.5574 - val_mse: 0.5574\n",
      "Epoch 138/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.5541 - val_mse: 0.5541\n",
      "Epoch 139/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.5559 - val_mse: 0.5559\n",
      "Epoch 140/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0239 - mse: 0.0239 - val_loss: 0.5568 - val_mse: 0.5568\n",
      "Epoch 141/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0227 - mse: 0.0227 - val_loss: 0.5511 - val_mse: 0.5511\n",
      "Epoch 142/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0221 - mse: 0.0221 - val_loss: 0.5541 - val_mse: 0.5541\n",
      "Epoch 143/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0232 - mse: 0.0232 - val_loss: 0.5533 - val_mse: 0.5533\n",
      "Epoch 144/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.5547 - val_mse: 0.5547\n",
      "Epoch 145/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0240 - mse: 0.0240 - val_loss: 0.5623 - val_mse: 0.5623\n",
      "Epoch 146/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 0.5505 - val_mse: 0.5505\n",
      "Epoch 147/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.5535 - val_mse: 0.5535\n",
      "Epoch 148/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 0.5580 - val_mse: 0.5580\n",
      "Epoch 149/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.5521 - val_mse: 0.5521\n",
      "Epoch 150/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0238 - mse: 0.0238 - val_loss: 0.5578 - val_mse: 0.5578\n",
      "Epoch 151/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0221 - mse: 0.0221 - val_loss: 0.5548 - val_mse: 0.5548\n",
      "Epoch 152/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 0.5528 - val_mse: 0.5528\n",
      "Epoch 153/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.5625 - val_mse: 0.5625\n",
      "Epoch 154/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0222 - mse: 0.0222 - val_loss: 0.5540 - val_mse: 0.5540\n",
      "Epoch 155/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.5567 - val_mse: 0.5567\n",
      "Epoch 156/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.5599 - val_mse: 0.5599\n",
      "Epoch 157/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.5524 - val_mse: 0.5524\n",
      "Epoch 158/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.5590 - val_mse: 0.5590\n",
      "Epoch 159/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 0.5568 - val_mse: 0.5568\n",
      "Epoch 160/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.5562 - val_mse: 0.5562\n",
      "Epoch 161/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.5545 - val_mse: 0.5545\n",
      "Epoch 162/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0221 - mse: 0.0221 - val_loss: 0.5601 - val_mse: 0.5601\n",
      "Epoch 163/500\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.0228 - mse: 0.0228 - val_loss: 0.5607 - val_mse: 0.5607\n",
      "Epoch 164/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.5581 - val_mse: 0.5581\n",
      "Epoch 165/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.5564 - val_mse: 0.5564\n",
      "Epoch 166/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.5530 - val_mse: 0.5530\n",
      "Epoch 167/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.5540 - val_mse: 0.5540\n",
      "Epoch 168/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.5568 - val_mse: 0.5568\n",
      "Epoch 169/500\n",
      "200/200 [==============================] - 4s 19ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 0.5585 - val_mse: 0.5585\n",
      "Epoch 170/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0212 - mse: 0.0212 - val_loss: 0.5593 - val_mse: 0.5593\n",
      "Epoch 171/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0212 - mse: 0.0212 - val_loss: 0.5519 - val_mse: 0.5519\n",
      "Epoch 172/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.5558 - val_mse: 0.5558\n",
      "Epoch 173/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.5528 - val_mse: 0.5528\n",
      "Epoch 174/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.5535 - val_mse: 0.5535\n",
      "Epoch 175/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.5566 - val_mse: 0.5566\n",
      "Epoch 176/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0197 - mse: 0.0197 - val_loss: 0.5556 - val_mse: 0.5556\n",
      "Epoch 177/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0212 - mse: 0.0212 - val_loss: 0.5537 - val_mse: 0.5537\n",
      "Epoch 178/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.5561 - val_mse: 0.5561\n",
      "Epoch 179/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0212 - mse: 0.0212 - val_loss: 0.5584 - val_mse: 0.5584\n",
      "Epoch 180/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0209 - mse: 0.0209 - val_loss: 0.5527 - val_mse: 0.5527\n",
      "Epoch 181/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.5535 - val_mse: 0.5535\n",
      "Epoch 182/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0207 - mse: 0.0207 - val_loss: 0.5581 - val_mse: 0.5581\n",
      "Epoch 183/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0212 - mse: 0.0212 - val_loss: 0.5618 - val_mse: 0.5618\n",
      "Epoch 184/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0219 - mse: 0.0219 - val_loss: 0.5640 - val_mse: 0.5640\n",
      "Epoch 185/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.5564 - val_mse: 0.5564\n",
      "Epoch 186/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.5564 - val_mse: 0.5564\n",
      "Epoch 187/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.5559 - val_mse: 0.5559\n",
      "Epoch 188/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.5641 - val_mse: 0.5641\n",
      "Epoch 189/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.5557 - val_mse: 0.5557\n",
      "Epoch 190/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0209 - mse: 0.0209 - val_loss: 0.5582 - val_mse: 0.5582\n",
      "Epoch 191/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.5544 - val_mse: 0.5544\n",
      "Epoch 192/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 0.5536 - val_mse: 0.5536\n",
      "Epoch 193/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 0.5561 - val_mse: 0.5561\n",
      "Epoch 194/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.5547 - val_mse: 0.5547\n",
      "Epoch 195/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.5583 - val_mse: 0.5583\n",
      "Epoch 196/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.5545 - val_mse: 0.5545\n",
      "Epoch 197/500\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.5573 - val_mse: 0.5573\n",
      "Epoch 198/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.5597 - val_mse: 0.5597\n",
      "Epoch 199/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.5580 - val_mse: 0.5580\n",
      "Epoch 200/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.5511 - val_mse: 0.5511\n",
      "Epoch 201/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0207 - mse: 0.0207 - val_loss: 0.5590 - val_mse: 0.5590\n",
      "Epoch 202/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.5562 - val_mse: 0.5562\n",
      "Epoch 203/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.5561 - val_mse: 0.5561\n",
      "Epoch 204/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0193 - mse: 0.0193 - val_loss: 0.5610 - val_mse: 0.5610\n",
      "Epoch 205/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0186 - mse: 0.0186 - val_loss: 0.5542 - val_mse: 0.5542\n",
      "Epoch 206/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.5559 - val_mse: 0.5559\n",
      "Epoch 207/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0193 - mse: 0.0193 - val_loss: 0.5586 - val_mse: 0.5586\n",
      "Epoch 208/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.5547 - val_mse: 0.5547\n",
      "Epoch 209/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.5633 - val_mse: 0.5633\n",
      "Epoch 210/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.5537 - val_mse: 0.5537\n",
      "Epoch 211/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.5594 - val_mse: 0.5594\n",
      "Epoch 212/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.5563 - val_mse: 0.5563\n",
      "Epoch 213/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.5578 - val_mse: 0.5578\n",
      "Epoch 214/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.5641 - val_mse: 0.5641\n",
      "Epoch 215/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.5549 - val_mse: 0.5549\n",
      "Epoch 216/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.5626 - val_mse: 0.5626\n",
      "Epoch 217/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.5562 - val_mse: 0.5562\n",
      "Epoch 218/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.5593 - val_mse: 0.5593\n",
      "Epoch 219/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.5551 - val_mse: 0.5551\n",
      "Epoch 220/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.5568 - val_mse: 0.5568\n",
      "Epoch 221/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.5614 - val_mse: 0.5614\n",
      "Epoch 222/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.5515 - val_mse: 0.5515\n",
      "Epoch 223/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.5579 - val_mse: 0.5579\n",
      "Epoch 224/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0199 - mse: 0.0199 - val_loss: 0.5547 - val_mse: 0.5547\n",
      "Epoch 225/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0194 - mse: 0.0194 - val_loss: 0.5515 - val_mse: 0.5515\n",
      "Epoch 226/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.5567 - val_mse: 0.5567\n",
      "Epoch 227/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.5604 - val_mse: 0.5604\n",
      "Epoch 228/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.5566 - val_mse: 0.5566\n",
      "Epoch 229/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.5537 - val_mse: 0.5537\n",
      "Epoch 230/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.5580 - val_mse: 0.5580\n",
      "Epoch 231/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.5533 - val_mse: 0.5533\n",
      "Epoch 232/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0193 - mse: 0.0193 - val_loss: 0.5541 - val_mse: 0.5541\n",
      "Epoch 233/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.5582 - val_mse: 0.5582\n",
      "Epoch 234/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.5499 - val_mse: 0.5499\n",
      "Epoch 235/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.5611 - val_mse: 0.5611\n",
      "Epoch 236/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.5638 - val_mse: 0.5638\n",
      "Epoch 237/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.5549 - val_mse: 0.5549\n",
      "Epoch 238/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.5542 - val_mse: 0.5542\n",
      "Epoch 239/500\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.5557 - val_mse: 0.5557\n",
      "Epoch 240/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.5543 - val_mse: 0.5543\n",
      "Epoch 241/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.5573 - val_mse: 0.5573\n",
      "Epoch 242/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.5543 - val_mse: 0.5543\n",
      "Epoch 243/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.5593 - val_mse: 0.5593\n",
      "Epoch 244/500\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.5577 - val_mse: 0.5577\n",
      "Epoch 245/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.5558 - val_mse: 0.5558\n",
      "Epoch 246/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.5568 - val_mse: 0.5568\n",
      "Epoch 247/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.5499 - val_mse: 0.5499\n",
      "Epoch 248/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.5529 - val_mse: 0.5529\n",
      "Epoch 249/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.5628 - val_mse: 0.5628\n",
      "Epoch 250/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0194 - mse: 0.0194 - val_loss: 0.5558 - val_mse: 0.5558\n",
      "Epoch 251/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.5530 - val_mse: 0.5530\n",
      "Epoch 252/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.5527 - val_mse: 0.5527\n",
      "Epoch 253/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.5584 - val_mse: 0.5584\n",
      "Epoch 254/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.5540 - val_mse: 0.5540\n",
      "Epoch 255/500\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.5560 - val_mse: 0.5560\n",
      "Epoch 256/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.5536 - val_mse: 0.5536\n",
      "Epoch 257/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.5548 - val_mse: 0.5548\n",
      "Epoch 258/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.5550 - val_mse: 0.5550\n",
      "Epoch 259/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0186 - mse: 0.0186 - val_loss: 0.5575 - val_mse: 0.5575\n",
      "Epoch 260/500\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.5545 - val_mse: 0.5545\n",
      "Epoch 261/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.5590 - val_mse: 0.5590\n",
      "Epoch 262/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.5596 - val_mse: 0.5596\n",
      "Epoch 263/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.5542 - val_mse: 0.5542\n",
      "Epoch 264/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.5545 - val_mse: 0.5545\n",
      "Epoch 265/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.5548 - val_mse: 0.5548\n",
      "Epoch 266/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.5599 - val_mse: 0.5599\n",
      "Epoch 267/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.5554 - val_mse: 0.5554\n",
      "Epoch 268/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.5527 - val_mse: 0.5527\n",
      "Epoch 269/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.5530 - val_mse: 0.5530\n",
      "Epoch 270/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.5568 - val_mse: 0.5568\n",
      "Epoch 271/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.5501 - val_mse: 0.5501\n",
      "Epoch 272/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.5600 - val_mse: 0.5600\n",
      "Epoch 273/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.5558 - val_mse: 0.5558\n",
      "Epoch 274/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.5534 - val_mse: 0.5534\n",
      "Epoch 275/500\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.5552 - val_mse: 0.5552\n",
      "Epoch 276/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.5584 - val_mse: 0.5584\n",
      "Epoch 277/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.5596 - val_mse: 0.5596\n",
      "Epoch 278/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.5583 - val_mse: 0.5583\n",
      "Epoch 279/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.5550 - val_mse: 0.5550\n",
      "Epoch 280/500\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.5568 - val_mse: 0.5568\n",
      "Epoch 281/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.5599 - val_mse: 0.5599\n",
      "Epoch 282/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.5559 - val_mse: 0.5559\n",
      "Epoch 283/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.5550 - val_mse: 0.5550\n",
      "Epoch 284/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.5565 - val_mse: 0.5565\n",
      "Epoch 285/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.5576 - val_mse: 0.5576\n",
      "Epoch 286/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.5569 - val_mse: 0.5569\n",
      "Epoch 287/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.5615 - val_mse: 0.5615\n",
      "Epoch 288/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.5574 - val_mse: 0.5574\n",
      "Epoch 289/500\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.5600 - val_mse: 0.5600\n",
      "Epoch 290/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.5570 - val_mse: 0.5570\n",
      "Epoch 291/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.5594 - val_mse: 0.5594\n",
      "Epoch 292/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.5589 - val_mse: 0.5589\n",
      "Epoch 293/500\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.5589 - val_mse: 0.5589\n",
      "Epoch 294/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.5586 - val_mse: 0.5586\n",
      "Epoch 295/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.5575 - val_mse: 0.5575\n",
      "Epoch 296/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.5572 - val_mse: 0.5572\n",
      "Epoch 297/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.5548 - val_mse: 0.5548\n",
      "Epoch 298/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.5585 - val_mse: 0.5585\n",
      "Epoch 299/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.5554 - val_mse: 0.5554\n",
      "Epoch 300/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.5587 - val_mse: 0.5587\n",
      "Epoch 301/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.5586 - val_mse: 0.5586\n",
      "Epoch 302/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.5533 - val_mse: 0.5533\n",
      "Epoch 303/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.5564 - val_mse: 0.5564\n",
      "Epoch 304/500\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.5605 - val_mse: 0.5605\n",
      "Epoch 305/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.5552 - val_mse: 0.5552\n",
      "Epoch 306/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.5546 - val_mse: 0.5546\n",
      "Epoch 307/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.5538 - val_mse: 0.5538\n",
      "Epoch 308/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.5555 - val_mse: 0.5555\n",
      "Epoch 309/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.5554 - val_mse: 0.5554\n",
      "Epoch 310/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.5585 - val_mse: 0.5585\n",
      "Epoch 311/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.5578 - val_mse: 0.5578\n",
      "Epoch 312/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.5637 - val_mse: 0.5637\n",
      "Epoch 313/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.5527 - val_mse: 0.5527\n",
      "Epoch 314/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.5582 - val_mse: 0.5582\n",
      "Epoch 315/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.5508 - val_mse: 0.5508\n",
      "Epoch 316/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.5560 - val_mse: 0.5560\n",
      "Epoch 317/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.5561 - val_mse: 0.5561\n",
      "Epoch 318/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.5569 - val_mse: 0.5569\n",
      "Epoch 319/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.5577 - val_mse: 0.5577\n",
      "Epoch 320/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.5551 - val_mse: 0.5551\n",
      "Epoch 321/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.5602 - val_mse: 0.5602\n",
      "Epoch 322/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.5569 - val_mse: 0.5569\n",
      "Epoch 323/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.5555 - val_mse: 0.5555\n",
      "Epoch 324/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.5595 - val_mse: 0.5595\n",
      "Epoch 325/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.5590 - val_mse: 0.5590\n",
      "Epoch 326/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.5552 - val_mse: 0.5552\n",
      "Epoch 327/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.5533 - val_mse: 0.5533\n",
      "Epoch 328/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.5552 - val_mse: 0.5552\n",
      "Epoch 329/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.5571 - val_mse: 0.5571\n",
      "Epoch 330/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.5516 - val_mse: 0.5516\n",
      "Epoch 331/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.5565 - val_mse: 0.5565\n",
      "Epoch 332/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.5518 - val_mse: 0.5518\n",
      "Epoch 333/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.5538 - val_mse: 0.5538\n",
      "Epoch 334/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.5559 - val_mse: 0.5559\n",
      "Epoch 335/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.5548 - val_mse: 0.5548\n",
      "Epoch 336/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.5567 - val_mse: 0.5567\n",
      "Epoch 337/500\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.5568 - val_mse: 0.5568\n",
      "Epoch 338/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.5579 - val_mse: 0.5579\n",
      "Epoch 339/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.5595 - val_mse: 0.5595\n",
      "Epoch 340/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.5532 - val_mse: 0.5532\n",
      "Epoch 341/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.5509 - val_mse: 0.5509\n",
      "Epoch 342/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.5505 - val_mse: 0.5505\n",
      "Epoch 343/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.5577 - val_mse: 0.5577\n",
      "Epoch 344/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.5596 - val_mse: 0.5596\n",
      "Epoch 345/500\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.5560 - val_mse: 0.5560\n",
      "Epoch 346/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.5565 - val_mse: 0.5565\n",
      "Epoch 347/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.5559 - val_mse: 0.5559\n",
      "Epoch 348/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.5591 - val_mse: 0.5591\n",
      "Epoch 349/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.5576 - val_mse: 0.5576\n",
      "Epoch 350/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.5595 - val_mse: 0.5595\n",
      "Epoch 351/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.5588 - val_mse: 0.5588\n",
      "Epoch 352/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.5562 - val_mse: 0.5562\n",
      "Epoch 353/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.5546 - val_mse: 0.5546\n",
      "Epoch 354/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.5521 - val_mse: 0.5521\n",
      "Epoch 355/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.5535 - val_mse: 0.5535\n",
      "Epoch 356/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.5525 - val_mse: 0.5525\n",
      "Epoch 357/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.5590 - val_mse: 0.5590\n",
      "Epoch 358/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.5598 - val_mse: 0.5598\n",
      "Epoch 359/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.5574 - val_mse: 0.5574\n",
      "Epoch 360/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.5554 - val_mse: 0.5554\n",
      "Epoch 361/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.5592 - val_mse: 0.5592\n",
      "Epoch 362/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.5550 - val_mse: 0.5550\n",
      "Epoch 363/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.5541 - val_mse: 0.5541\n",
      "Epoch 364/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.5564 - val_mse: 0.5564\n",
      "Epoch 365/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.5576 - val_mse: 0.5576\n",
      "Epoch 366/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.5523 - val_mse: 0.5523\n",
      "Epoch 367/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.5540 - val_mse: 0.5540\n",
      "Epoch 368/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.5573 - val_mse: 0.5573\n",
      "Epoch 369/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.5587 - val_mse: 0.5587\n",
      "Epoch 370/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.5569 - val_mse: 0.5569\n",
      "Epoch 371/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.5560 - val_mse: 0.5560\n",
      "Epoch 372/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.5553 - val_mse: 0.5553\n",
      "Epoch 373/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.5568 - val_mse: 0.5568\n",
      "Epoch 374/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.5593 - val_mse: 0.5593\n",
      "Epoch 375/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.5566 - val_mse: 0.5566\n",
      "Epoch 376/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.5558 - val_mse: 0.5558\n",
      "Epoch 377/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.5574 - val_mse: 0.5574\n",
      "Epoch 378/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.5563 - val_mse: 0.5563\n",
      "Epoch 379/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.5551 - val_mse: 0.5551\n",
      "Epoch 380/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.5555 - val_mse: 0.5555\n",
      "Epoch 381/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.5592 - val_mse: 0.5592\n",
      "Epoch 382/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.5568 - val_mse: 0.5568\n",
      "Epoch 383/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.5577 - val_mse: 0.5577\n",
      "Epoch 384/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.5603 - val_mse: 0.5603\n",
      "Epoch 385/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.5565 - val_mse: 0.5565\n",
      "Epoch 386/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.5567 - val_mse: 0.5567\n",
      "Epoch 387/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.5555 - val_mse: 0.5555\n",
      "Epoch 388/500\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.5586 - val_mse: 0.5586\n",
      "Epoch 389/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.5562 - val_mse: 0.5562\n",
      "Epoch 390/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.5568 - val_mse: 0.5568\n",
      "Epoch 391/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.5508 - val_mse: 0.5508\n",
      "Epoch 392/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.5586 - val_mse: 0.5586\n",
      "Epoch 393/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.5536 - val_mse: 0.5536\n",
      "Epoch 394/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.5562 - val_mse: 0.5562\n",
      "Epoch 395/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.5499 - val_mse: 0.5499\n",
      "Epoch 396/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.5562 - val_mse: 0.5562\n",
      "Epoch 397/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.5558 - val_mse: 0.5558\n",
      "Epoch 398/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.5558 - val_mse: 0.5558\n",
      "Epoch 399/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.5580 - val_mse: 0.5580\n",
      "Epoch 400/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.5536 - val_mse: 0.5536\n",
      "Epoch 401/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.5549 - val_mse: 0.5549\n",
      "Epoch 402/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.5579 - val_mse: 0.5579\n",
      "Epoch 403/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.5536 - val_mse: 0.5536\n",
      "Epoch 404/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.5523 - val_mse: 0.5523\n",
      "Epoch 405/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.5585 - val_mse: 0.5585\n",
      "Epoch 406/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.5575 - val_mse: 0.5575\n",
      "Epoch 407/500\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.5566 - val_mse: 0.5566\n",
      "Epoch 408/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.5594 - val_mse: 0.5594\n",
      "Epoch 409/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.5595 - val_mse: 0.5595\n",
      "Epoch 410/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.5574 - val_mse: 0.5574\n",
      "Epoch 411/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.5566 - val_mse: 0.5566\n",
      "Epoch 412/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.5599 - val_mse: 0.5599\n",
      "Epoch 413/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.5573 - val_mse: 0.5573\n",
      "Epoch 414/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.5579 - val_mse: 0.5579\n",
      "Epoch 415/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.5573 - val_mse: 0.5573\n",
      "Epoch 416/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.5557 - val_mse: 0.5557\n",
      "Epoch 417/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.5574 - val_mse: 0.5574\n",
      "Epoch 418/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.5597 - val_mse: 0.5597\n",
      "Epoch 419/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.5574 - val_mse: 0.5574\n",
      "Epoch 420/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.5540 - val_mse: 0.5540\n",
      "Epoch 421/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.5576 - val_mse: 0.5576\n",
      "Epoch 422/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.5582 - val_mse: 0.5582\n",
      "Epoch 423/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.5641 - val_mse: 0.5641\n",
      "Epoch 424/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.5606 - val_mse: 0.5606\n",
      "Epoch 425/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.5605 - val_mse: 0.5605\n",
      "Epoch 426/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.5529 - val_mse: 0.5529\n",
      "Epoch 427/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.5574 - val_mse: 0.5574\n",
      "Epoch 428/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.5574 - val_mse: 0.5574\n",
      "Epoch 429/500\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.5571 - val_mse: 0.5571\n",
      "Epoch 430/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.5560 - val_mse: 0.5560\n",
      "Epoch 431/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.5526 - val_mse: 0.5526\n",
      "Epoch 432/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.5589 - val_mse: 0.5589\n",
      "Epoch 433/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.5581 - val_mse: 0.5581\n",
      "Epoch 434/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.5567 - val_mse: 0.5567\n",
      "Epoch 435/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.5570 - val_mse: 0.5570\n",
      "Epoch 436/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.5535 - val_mse: 0.5535\n",
      "Epoch 437/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.5557 - val_mse: 0.5557\n",
      "Epoch 438/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.5531 - val_mse: 0.5531\n",
      "Epoch 439/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.5578 - val_mse: 0.5578\n",
      "Epoch 440/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.5562 - val_mse: 0.5562\n",
      "Epoch 441/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.5619 - val_mse: 0.5619\n",
      "Epoch 442/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.5551 - val_mse: 0.5551\n",
      "Epoch 443/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.5549 - val_mse: 0.5549\n",
      "Epoch 444/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.5595 - val_mse: 0.5595\n",
      "Epoch 445/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.5578 - val_mse: 0.5578\n",
      "Epoch 446/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.5607 - val_mse: 0.5607\n",
      "Epoch 447/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.5600 - val_mse: 0.5600\n",
      "Epoch 448/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.5585 - val_mse: 0.5585\n",
      "Epoch 449/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.5593 - val_mse: 0.5593\n",
      "Epoch 450/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.5570 - val_mse: 0.5570\n",
      "Epoch 451/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.5583 - val_mse: 0.5583\n",
      "Epoch 452/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.5593 - val_mse: 0.5593\n",
      "Epoch 453/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.5574 - val_mse: 0.5574\n",
      "Epoch 454/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.5601 - val_mse: 0.5601\n",
      "Epoch 455/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.5594 - val_mse: 0.5594\n",
      "Epoch 456/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.5563 - val_mse: 0.5563\n",
      "Epoch 457/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.5581 - val_mse: 0.5581\n",
      "Epoch 458/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.5595 - val_mse: 0.5595\n",
      "Epoch 459/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.5572 - val_mse: 0.5572\n",
      "Epoch 460/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.5586 - val_mse: 0.5586\n",
      "Epoch 461/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.5567 - val_mse: 0.5567\n",
      "Epoch 462/500\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.5573 - val_mse: 0.5573\n",
      "Epoch 463/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.5587 - val_mse: 0.5587\n",
      "Epoch 464/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.5575 - val_mse: 0.5575\n",
      "Epoch 465/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.5592 - val_mse: 0.5592\n",
      "Epoch 466/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.5604 - val_mse: 0.5604\n",
      "Epoch 467/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.5588 - val_mse: 0.5588\n",
      "Epoch 468/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.5563 - val_mse: 0.5563\n",
      "Epoch 469/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.5569 - val_mse: 0.5569\n",
      "Epoch 470/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.5525 - val_mse: 0.5525\n",
      "Epoch 471/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.5582 - val_mse: 0.5582\n",
      "Epoch 472/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.5591 - val_mse: 0.5591\n",
      "Epoch 473/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.5615 - val_mse: 0.5615\n",
      "Epoch 474/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.5613 - val_mse: 0.5613\n",
      "Epoch 475/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.5597 - val_mse: 0.5597\n",
      "Epoch 476/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.5568 - val_mse: 0.5568\n",
      "Epoch 477/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.5564 - val_mse: 0.5564\n",
      "Epoch 478/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.5539 - val_mse: 0.5539\n",
      "Epoch 479/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.5590 - val_mse: 0.5590\n",
      "Epoch 480/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.5570 - val_mse: 0.5570\n",
      "Epoch 481/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.5541 - val_mse: 0.5541\n",
      "Epoch 482/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.5533 - val_mse: 0.5533\n",
      "Epoch 483/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.5566 - val_mse: 0.5566\n",
      "Epoch 484/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.5551 - val_mse: 0.5551\n",
      "Epoch 485/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.5536 - val_mse: 0.5536\n",
      "Epoch 486/500\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.5548 - val_mse: 0.5548\n",
      "Epoch 487/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.5575 - val_mse: 0.5575\n",
      "Epoch 488/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.5531 - val_mse: 0.5531\n",
      "Epoch 489/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.5583 - val_mse: 0.5583\n",
      "Epoch 490/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.5589 - val_mse: 0.5589\n",
      "Epoch 491/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.5584 - val_mse: 0.5584\n",
      "Epoch 492/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.5597 - val_mse: 0.5597\n",
      "Epoch 493/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.5541 - val_mse: 0.5541\n",
      "Epoch 494/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.5522 - val_mse: 0.5522\n",
      "Epoch 495/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.5545 - val_mse: 0.5545\n",
      "Epoch 496/500\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.5535 - val_mse: 0.5535\n",
      "Epoch 497/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.5587 - val_mse: 0.5587\n",
      "Epoch 498/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.5609 - val_mse: 0.5609\n",
      "Epoch 499/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.5607 - val_mse: 0.5607\n",
      "Epoch 500/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.5614 - val_mse: 0.5614\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prefix = \"new30\"\n",
    "for i in range(10):\n",
    "    filename = f\"{prefix}/{prefix}_{i}.csv\"\n",
    "    # csvファイルの読み込み\n",
    "    if i == 0:\n",
    "        df = pd.read_csv(filename, header=0)\n",
    "        df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "    else:\n",
    "        tmp = pd.read_csv(filename, header=0)\n",
    "        tmp = tmp.drop(\"Unnamed: 0\", axis=1)\n",
    "        df = pd.concat([df, tmp], axis=0)\n",
    "\n",
    "cherry_pick(df)\n",
    "\n",
    "\n",
    "#dataframeをnumpy配列に変換\n",
    "data = df.values\n",
    "#深層学習の入力データと出力データに分ける\n",
    "# 入力データ\n",
    "len_test = 8\n",
    "x = data[:, len_test:]\n",
    "\n",
    "# 出力データ\n",
    "y = data[:, :len_test]\n",
    "#入力データの正規化\n",
    "x = (x - x.mean()) / x.std()\n",
    "\n",
    "\n",
    "#出力データのMin-Max正規化\n",
    "y = (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "#入力データの次元数\n",
    "input_dim = x.shape[1]\n",
    "\n",
    "#出力データの次元数\n",
    "output_dim = y.shape[1]\n",
    "\n",
    "#入力データの数\n",
    "n = x.shape[0]\n",
    "\n",
    "# 出力データ\n",
    "y = data[:, :len_test]\n",
    "#入力データの正規化\n",
    "x = (x - x.mean()) / x.std()\n",
    "\n",
    "#出力データの正規化\n",
    "y = (y - y.mean()) / y.std()\n",
    "\n",
    "#入力データの次元数\n",
    "input_dim = x.shape[1]\n",
    "\n",
    "#出力データの次元数\n",
    "output_dim = y.shape[1]\n",
    "\n",
    "#入力データの数\n",
    "n = x.shape[0]\n",
    "\n",
    "#学習データとテストデータに分ける\n",
    "#学習データ\n",
    "x_train = x[:int(n*0.8)]\n",
    "y_train = y[:int(n*0.8)]\n",
    "\n",
    "#テストデータ\n",
    "x_test = x[int(n*0.8):]\n",
    "y_test = y[int(n*0.8):]\n",
    "\n",
    "# モデルの定義 \n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(201, activation='relu', input_shape=(input_dim,)),\n",
    "    # ドロップアウト\n",
    "    #tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(236, activation='relu', input_shape=(input_dim,)),\n",
    "    #ドロップアウト\n",
    "    #tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(150, activation='relu'),\n",
    "    #tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(output_dim)\n",
    "])\n",
    "\n",
    "def compile():\n",
    "    #モデルのコンパイル\n",
    "    model.compile(optimizer='adam',\n",
    "                    loss='mse',\n",
    "                    metrics=['mse'])\n",
    "\n",
    "    #モデルの学習\n",
    "    history = model.fit(x_train, y_train, epochs=500, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    #学習の様子をグラフに描画\n",
    "    #損失関数の値\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [MPG]')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    #表示\n",
    "    plt.show()\n",
    "    return model,history\n",
    "\n",
    "model,history = compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    # ハイパーパラメータのサンプリング\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        n_units = trial.suggest_int('n_units_l{}'.format(i), 8, 256)\n",
    "        layers.append(tf.keras.layers.Dense(n_units, activation='relu'))\n",
    "    layers.append(tf.keras.layers.Dense(output_dim))\n",
    "    model = tf.keras.models.Sequential(layers)\n",
    "\n",
    "    # モデルのコンパイル\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='rmse',\n",
    "                metrics=['mse'])\n",
    "\n",
    "    # モデルの学習\n",
    "    history = model.fit(x_train, y_train, epochs=1000, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    # 損失関数の値\n",
    "    return history.history['val_loss'][-1]\n",
    "\n",
    "\n",
    "# チューニングを行う\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 86ms/step\n",
      "[[-0.43889177  1.02085     0.5219689  -0.52302784  0.21971996 -0.02781768\n",
      "  -0.41415468  0.44776008]]\n",
      "[[-1.3134483   0.72089496  0.36189321 -0.59544479 -1.0741138   1.43889847\n",
      "  -0.71511204  0.60122771]]\n"
     ]
    }
   ],
   "source": [
    "#1行目の値を実際に計算\n",
    "print(model.predict(x_test[:1]))\n",
    "#実際の値\n",
    "print(y_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# 片対数プロット\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39m# plotの上限を設定\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# 片対数プロット\n",
    "\n",
    "plt.plot(history[\"loss\"], label=\"loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "# plotの上限を設定\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "ax = plt.gca()\n",
    "\n",
    "# 軸ラベルを追加\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelの保存\n",
    "prefix = \"neural\"\n",
    "model.save(f\"models/{prefix}.h5\")\n",
    "# 学習経過の保存\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(f\"models/{prefix}.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-fluid-python3-_nSsHetM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c7e84feb875d0a630bed22fa394cb9e9d77c800863796b67237a2f1657097b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
